{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BSC ID</th>\n",
       "      <th>Retrospective1/Prospective2</th>\n",
       "      <th>Diagnosis_simplified</th>\n",
       "      <th>ACAN_bx</th>\n",
       "      <th>ADAR_bx</th>\n",
       "      <th>ADARB1_bx</th>\n",
       "      <th>AFP_bx</th>\n",
       "      <th>ALDH2_bx</th>\n",
       "      <th>ANGPT2_bx</th>\n",
       "      <th>APCDD1_bx</th>\n",
       "      <th>...</th>\n",
       "      <th>ACTG1_bx</th>\n",
       "      <th>B2M_bx</th>\n",
       "      <th>EEF1A1_bx</th>\n",
       "      <th>GAPDH_bx</th>\n",
       "      <th>MBD2_bx</th>\n",
       "      <th>NCOR1_bx</th>\n",
       "      <th>PNN_bx</th>\n",
       "      <th>RHOT2_bx</th>\n",
       "      <th>SNUPN_bx</th>\n",
       "      <th>TBP_bx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "      <td>HCC</td>\n",
       "      <td>318.29</td>\n",
       "      <td>1748.35</td>\n",
       "      <td>381.21</td>\n",
       "      <td>2295.44</td>\n",
       "      <td>2211.32</td>\n",
       "      <td>45.41</td>\n",
       "      <td>186.26</td>\n",
       "      <td>...</td>\n",
       "      <td>36545.45</td>\n",
       "      <td>12273.24</td>\n",
       "      <td>57654.07</td>\n",
       "      <td>65182.96</td>\n",
       "      <td>716.09</td>\n",
       "      <td>1026.02</td>\n",
       "      <td>2140.29</td>\n",
       "      <td>1016.71</td>\n",
       "      <td>315.55</td>\n",
       "      <td>441.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>HB</td>\n",
       "      <td>23.44</td>\n",
       "      <td>1794.59</td>\n",
       "      <td>337.13</td>\n",
       "      <td>267.34</td>\n",
       "      <td>5305.16</td>\n",
       "      <td>56.89</td>\n",
       "      <td>921.45</td>\n",
       "      <td>...</td>\n",
       "      <td>17736.22</td>\n",
       "      <td>21511.04</td>\n",
       "      <td>67905.40</td>\n",
       "      <td>12983.08</td>\n",
       "      <td>647.37</td>\n",
       "      <td>543.48</td>\n",
       "      <td>2229.07</td>\n",
       "      <td>916.82</td>\n",
       "      <td>199.92</td>\n",
       "      <td>264.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>HB</td>\n",
       "      <td>44.07</td>\n",
       "      <td>2222.78</td>\n",
       "      <td>422.76</td>\n",
       "      <td>8484.03</td>\n",
       "      <td>4219.69</td>\n",
       "      <td>119.10</td>\n",
       "      <td>315.81</td>\n",
       "      <td>...</td>\n",
       "      <td>16984.33</td>\n",
       "      <td>30481.94</td>\n",
       "      <td>64341.23</td>\n",
       "      <td>24678.02</td>\n",
       "      <td>1127.14</td>\n",
       "      <td>649.27</td>\n",
       "      <td>1941.39</td>\n",
       "      <td>638.10</td>\n",
       "      <td>221.83</td>\n",
       "      <td>321.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>174</td>\n",
       "      <td>2</td>\n",
       "      <td>HB</td>\n",
       "      <td>92.08</td>\n",
       "      <td>1828.75</td>\n",
       "      <td>273.52</td>\n",
       "      <td>2908.56</td>\n",
       "      <td>2421.77</td>\n",
       "      <td>217.08</td>\n",
       "      <td>801.26</td>\n",
       "      <td>...</td>\n",
       "      <td>29588.53</td>\n",
       "      <td>60920.25</td>\n",
       "      <td>16573.95</td>\n",
       "      <td>6999.96</td>\n",
       "      <td>151.02</td>\n",
       "      <td>191.93</td>\n",
       "      <td>3182.01</td>\n",
       "      <td>11279.62</td>\n",
       "      <td>246.47</td>\n",
       "      <td>122.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>175</td>\n",
       "      <td>2</td>\n",
       "      <td>HB</td>\n",
       "      <td>248.18</td>\n",
       "      <td>10735.79</td>\n",
       "      <td>481.26</td>\n",
       "      <td>1181.16</td>\n",
       "      <td>5289.23</td>\n",
       "      <td>338.10</td>\n",
       "      <td>6296.93</td>\n",
       "      <td>...</td>\n",
       "      <td>20953.08</td>\n",
       "      <td>60827.22</td>\n",
       "      <td>44292.82</td>\n",
       "      <td>36202.39</td>\n",
       "      <td>1201.67</td>\n",
       "      <td>1141.87</td>\n",
       "      <td>2272.53</td>\n",
       "      <td>742.75</td>\n",
       "      <td>251.25</td>\n",
       "      <td>455.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>363</td>\n",
       "      <td>2</td>\n",
       "      <td>HCC</td>\n",
       "      <td>191.76</td>\n",
       "      <td>5809.28</td>\n",
       "      <td>894.08</td>\n",
       "      <td>52.87</td>\n",
       "      <td>4548.74</td>\n",
       "      <td>285.03</td>\n",
       "      <td>288.30</td>\n",
       "      <td>...</td>\n",
       "      <td>13575.93</td>\n",
       "      <td>85542.37</td>\n",
       "      <td>57056.04</td>\n",
       "      <td>104427.17</td>\n",
       "      <td>1254.53</td>\n",
       "      <td>718.50</td>\n",
       "      <td>1494.80</td>\n",
       "      <td>839.66</td>\n",
       "      <td>333.59</td>\n",
       "      <td>1193.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>375</td>\n",
       "      <td>2</td>\n",
       "      <td>HB</td>\n",
       "      <td>266.05</td>\n",
       "      <td>2154.46</td>\n",
       "      <td>561.09</td>\n",
       "      <td>3683.40</td>\n",
       "      <td>2022.50</td>\n",
       "      <td>612.37</td>\n",
       "      <td>32647.56</td>\n",
       "      <td>...</td>\n",
       "      <td>31790.94</td>\n",
       "      <td>34743.43</td>\n",
       "      <td>84619.39</td>\n",
       "      <td>46024.46</td>\n",
       "      <td>1159.97</td>\n",
       "      <td>1317.30</td>\n",
       "      <td>1629.73</td>\n",
       "      <td>561.62</td>\n",
       "      <td>462.60</td>\n",
       "      <td>664.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>380</td>\n",
       "      <td>2</td>\n",
       "      <td>HB</td>\n",
       "      <td>266.01</td>\n",
       "      <td>1939.72</td>\n",
       "      <td>717.56</td>\n",
       "      <td>30432.76</td>\n",
       "      <td>1544.75</td>\n",
       "      <td>478.42</td>\n",
       "      <td>1853.14</td>\n",
       "      <td>...</td>\n",
       "      <td>23119.62</td>\n",
       "      <td>29761.69</td>\n",
       "      <td>69854.81</td>\n",
       "      <td>53577.06</td>\n",
       "      <td>1243.97</td>\n",
       "      <td>661.46</td>\n",
       "      <td>1646.96</td>\n",
       "      <td>627.74</td>\n",
       "      <td>333.36</td>\n",
       "      <td>405.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>386</td>\n",
       "      <td>2</td>\n",
       "      <td>HB</td>\n",
       "      <td>143.68</td>\n",
       "      <td>3502.59</td>\n",
       "      <td>537.11</td>\n",
       "      <td>3209.49</td>\n",
       "      <td>7722.82</td>\n",
       "      <td>304.07</td>\n",
       "      <td>7141.68</td>\n",
       "      <td>...</td>\n",
       "      <td>20908.16</td>\n",
       "      <td>48800.93</td>\n",
       "      <td>74987.34</td>\n",
       "      <td>33916.04</td>\n",
       "      <td>1247.33</td>\n",
       "      <td>731.60</td>\n",
       "      <td>1636.05</td>\n",
       "      <td>587.09</td>\n",
       "      <td>335.64</td>\n",
       "      <td>367.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>390</td>\n",
       "      <td>2</td>\n",
       "      <td>HB</td>\n",
       "      <td>149.60</td>\n",
       "      <td>2059.31</td>\n",
       "      <td>485.85</td>\n",
       "      <td>1264.89</td>\n",
       "      <td>1976.05</td>\n",
       "      <td>532.85</td>\n",
       "      <td>20380.37</td>\n",
       "      <td>...</td>\n",
       "      <td>23040.32</td>\n",
       "      <td>63167.50</td>\n",
       "      <td>65210.43</td>\n",
       "      <td>43769.52</td>\n",
       "      <td>1385.54</td>\n",
       "      <td>595.63</td>\n",
       "      <td>1848.91</td>\n",
       "      <td>537.80</td>\n",
       "      <td>267.46</td>\n",
       "      <td>421.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BSC ID  Retrospective1/Prospective2 Diagnosis_simplified  ACAN_bx  \\\n",
       "167     171                            2                  HCC   318.29   \n",
       "168     172                            2                   HB    23.44   \n",
       "169     173                            2                   HB    44.07   \n",
       "170     174                            2                   HB    92.08   \n",
       "171     175                            2                   HB   248.18   \n",
       "..      ...                          ...                  ...      ...   \n",
       "331     363                            2                  HCC   191.76   \n",
       "338     375                            2                   HB   266.05   \n",
       "339     380                            2                   HB   266.01   \n",
       "340     386                            2                   HB   143.68   \n",
       "343     390                            2                   HB   149.60   \n",
       "\n",
       "      ADAR_bx  ADARB1_bx    AFP_bx  ALDH2_bx  ANGPT2_bx  APCDD1_bx  ...  \\\n",
       "167   1748.35     381.21   2295.44   2211.32      45.41     186.26  ...   \n",
       "168   1794.59     337.13    267.34   5305.16      56.89     921.45  ...   \n",
       "169   2222.78     422.76   8484.03   4219.69     119.10     315.81  ...   \n",
       "170   1828.75     273.52   2908.56   2421.77     217.08     801.26  ...   \n",
       "171  10735.79     481.26   1181.16   5289.23     338.10    6296.93  ...   \n",
       "..        ...        ...       ...       ...        ...        ...  ...   \n",
       "331   5809.28     894.08     52.87   4548.74     285.03     288.30  ...   \n",
       "338   2154.46     561.09   3683.40   2022.50     612.37   32647.56  ...   \n",
       "339   1939.72     717.56  30432.76   1544.75     478.42    1853.14  ...   \n",
       "340   3502.59     537.11   3209.49   7722.82     304.07    7141.68  ...   \n",
       "343   2059.31     485.85   1264.89   1976.05     532.85   20380.37  ...   \n",
       "\n",
       "     ACTG1_bx    B2M_bx  EEF1A1_bx   GAPDH_bx  MBD2_bx  NCOR1_bx   PNN_bx  \\\n",
       "167  36545.45  12273.24   57654.07   65182.96   716.09   1026.02  2140.29   \n",
       "168  17736.22  21511.04   67905.40   12983.08   647.37    543.48  2229.07   \n",
       "169  16984.33  30481.94   64341.23   24678.02  1127.14    649.27  1941.39   \n",
       "170  29588.53  60920.25   16573.95    6999.96   151.02    191.93  3182.01   \n",
       "171  20953.08  60827.22   44292.82   36202.39  1201.67   1141.87  2272.53   \n",
       "..        ...       ...        ...        ...      ...       ...      ...   \n",
       "331  13575.93  85542.37   57056.04  104427.17  1254.53    718.50  1494.80   \n",
       "338  31790.94  34743.43   84619.39   46024.46  1159.97   1317.30  1629.73   \n",
       "339  23119.62  29761.69   69854.81   53577.06  1243.97    661.46  1646.96   \n",
       "340  20908.16  48800.93   74987.34   33916.04  1247.33    731.60  1636.05   \n",
       "343  23040.32  63167.50   65210.43   43769.52  1385.54    595.63  1848.91   \n",
       "\n",
       "     RHOT2_bx  SNUPN_bx   TBP_bx  \n",
       "167   1016.71    315.55   441.38  \n",
       "168    916.82    199.92   264.27  \n",
       "169    638.10    221.83   321.61  \n",
       "170  11279.62    246.47   122.92  \n",
       "171    742.75    251.25   455.63  \n",
       "..        ...       ...      ...  \n",
       "331    839.66    333.59  1193.40  \n",
       "338    561.62    462.60   664.44  \n",
       "339    627.74    333.36   405.55  \n",
       "340    587.09    335.64   367.24  \n",
       "343    537.80    267.46   421.75  \n",
       "\n",
       "[102 rows x 231 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Nueva global_180722.csv\")\n",
    "\n",
    "nanostring_bx_prosp = df.loc[:, \"ACAN_bx\": \"TBP_bx\"]\n",
    "df_labels = df.loc[:, \"BSC ID\" : \"Diagnosis_simplified\"]\n",
    "df_labels = df_labels.drop(\"Diagnosis_DEF_4cat\", axis=1)\n",
    "subset = pd.concat([df_labels, nanostring_bx_prosp,], axis=1)\n",
    "subset = subset.loc[subset[\"Retrospective1/Prospective2\"] == 2]\n",
    "subset = subset.dropna()\n",
    "subset = subset.loc[subset[\"Diagnosis_simplified\"].isin([\"HB\", \"HCC\", \"NOS\"])]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data for unbalance classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = subset.drop(['BSC ID', 'Retrospective1/Prospective2'], axis = 1)\n",
    "subset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = subset.drop('Diagnosis_simplified', axis=1)\n",
    "y = subset['Diagnosis_simplified']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X before SMOTE: (102, 228)\n",
      "Shape of X after SMOTE: (255, 228)\n",
      "\n",
      "Balance of positive and negative classes (%):\n",
      "NOS    33.333333\n",
      "HB     33.333333\n",
      "HCC    33.333333\n",
      "Name: Diagnosis_simplified, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(k_neighbors=4)\n",
    "\n",
    "X_sm, y_sm = sm.fit_resample(X, y)\n",
    "\n",
    "print(f'''Shape of X before SMOTE: {X.shape}\n",
    "Shape of X after SMOTE: {X_sm.shape}''')\n",
    "\n",
    "print('\\nBalance of positive and negative classes (%):')\n",
    "print(y_sm.value_counts(normalize=True) * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sm = X_sm.drop('labels',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model hyperparameters selction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune regularization for multinomial logistic regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#Dataset\n",
    "X = subset.drop('Diagnosis_simplified', axis=1)\n",
    "y = subset['Diagnosis_simplified']\n",
    "\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f' % p\n",
    "\t\t# turn off penalty in some cases\n",
    "\t\tif p == 0.0:\n",
    "\t\t\t# no penalty in this case\n",
    "\t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
    "\t\telse:\n",
    "\t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
    "\treturn models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t# define the evaluation procedure\n",
    "\tcv = KFold(n_splits=10)\n",
    "\t# evaluate the model\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\t# evaluate the model and collect the scores\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\t# store the results\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\t# summarize progress along the way\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "# define the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C = 0.1, penalty=\"l2\", max_iter = 1000000)\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X_sm, y_sm)\n",
    "\n",
    "\n",
    "# predict a multinomial probability distribution\n",
    "yhat = model.predict_proba(X)\n",
    "# summarize the predicted probabilities\n",
    "print('Predicted Probabilities: %s' % yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the class label\n",
    "yhat = model.predict(X)\n",
    "# summarize the predicted class\n",
    "print(\"Predicted Class: \",  yhat)\n",
    "print(pd.DataFrame(yhat).iloc[:,0].value_counts())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(pd.DataFrame(yhat == y).iloc[:,0].value_counts())\n",
    "\n",
    "score = cross_val_score(model, X, y, scoring='accuracy', n_jobs=-1)\n",
    "print('>%s %.3f (%.3f)' % (name, mean(score), std(score)))\n",
    "accuracy_score(y, yhat, sample_weight= y.map(dicc).to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_sm[\"ACAN_bx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3013fc88d73a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_sm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ACAN_bx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_X\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX_sm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ACAN_bx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0my_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "train_X = np.array( X_sm[\"ACAN_bx\"].sample(frac=0.7,replace=False)).reshape(-1,1)\n",
    "train_y = y_sm.sample(frac=0.7,replace=False)\n",
    "test_X= X_sm[\"ACAN_bx\"].drop(train_X.index)\n",
    "test_y =y_sm.drop(train_y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[178.27052116  36.54815482  73.24436393  40.22008319  77.30654845\n 281.44        31.0603535   39.65649257 183.1550891   83.98975763\n  50.75        59.12888347 143.68        52.12        35.9219127\n 157.66        59.58578467  37.07510546  58.97242702  55.43892301\n  21.93        81.51       698.78        87.23        87.53989701\n 119.26253161  65.66413155 318.29        84.69548369 362.96\n  81.37073965 104.68037661  45.78        74.09        82.75\n  69.58611166  40.71672634 426.53        34.96       148.16\n  53.55        58.82        99.01        46.74        64.49\n  81.11        73.43316404  86.31112587  97.55        68.13\n  81.06828018  59.27145827 318.23087807 266.05        49.82259647\n  36.62316852  77.7131685  124.4         52.93861483  33.64\n  76.06435998  26.79       119.82128699  75.64495876  65.6835375\n  92.08        90.0474429  207.83        34.62        75.21912831\n  79.16700249  36.39336849  47.08        43.56641457  77.71849118\n  78.71151195  74.00468938 266.01        76.04        72.65866683\n  65.58        82.79495135  90.48        81.84        58.70124927\n 132.26       140.92828044  49.913401    69.97173343  59.87184963\n  35.42       782.09        64.00691265  56.52645322 124.22\n 149.6         79.56597661  71.68501508  52.18       117.33826344\n 159.63479393  84.74409303 534.37        37.35938073  84.26279058\n  74.3149686   86.3517823   59.15401727  52.84064018  81.73\n 124.66        42.67725148  90.06       110.81907039  73.63540456\n  63.82143048 164.19        42.25710627  34.47868897  49.47\n  76.87032901  35.43        79.97323517  41.7765412   38.05\n  56.43632973  96.48600798  76.19830424 121.01911559  93.13398327\n  89.1         65.20116315 166.75967072 119.74592635  59.66\n  84.78       161.39778958  42.62        84.18917627 186.72885031\n 159.77679191  36.55390861 329.17        59.02930369  85.00082653\n  46.88       143.12557289  80.8         86.66598109 207.64073281\n 397.23        80.98029528 117.77       117.33       101.63749557\n  62.65        86.11689047  76.59903555 124.49        60.08\n 103.41848446 230.6         43.4556468   82.96950594  85.57410146\n  52.69879882  53.42        36.09575685  53.10587067 208.77\n  45.31166756 248.18        52.15109978 179.39        40.06335019\n 106.52        40.52        36.53135678].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-50c79a4f571c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# fit the model on the whole dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1136\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         )\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1075\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    880\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[178.27052116  36.54815482  73.24436393  40.22008319  77.30654845\n 281.44        31.0603535   39.65649257 183.1550891   83.98975763\n  50.75        59.12888347 143.68        52.12        35.9219127\n 157.66        59.58578467  37.07510546  58.97242702  55.43892301\n  21.93        81.51       698.78        87.23        87.53989701\n 119.26253161  65.66413155 318.29        84.69548369 362.96\n  81.37073965 104.68037661  45.78        74.09        82.75\n  69.58611166  40.71672634 426.53        34.96       148.16\n  53.55        58.82        99.01        46.74        64.49\n  81.11        73.43316404  86.31112587  97.55        68.13\n  81.06828018  59.27145827 318.23087807 266.05        49.82259647\n  36.62316852  77.7131685  124.4         52.93861483  33.64\n  76.06435998  26.79       119.82128699  75.64495876  65.6835375\n  92.08        90.0474429  207.83        34.62        75.21912831\n  79.16700249  36.39336849  47.08        43.56641457  77.71849118\n  78.71151195  74.00468938 266.01        76.04        72.65866683\n  65.58        82.79495135  90.48        81.84        58.70124927\n 132.26       140.92828044  49.913401    69.97173343  59.87184963\n  35.42       782.09        64.00691265  56.52645322 124.22\n 149.6         79.56597661  71.68501508  52.18       117.33826344\n 159.63479393  84.74409303 534.37        37.35938073  84.26279058\n  74.3149686   86.3517823   59.15401727  52.84064018  81.73\n 124.66        42.67725148  90.06       110.81907039  73.63540456\n  63.82143048 164.19        42.25710627  34.47868897  49.47\n  76.87032901  35.43        79.97323517  41.7765412   38.05\n  56.43632973  96.48600798  76.19830424 121.01911559  93.13398327\n  89.1         65.20116315 166.75967072 119.74592635  59.66\n  84.78       161.39778958  42.62        84.18917627 186.72885031\n 159.77679191  36.55390861 329.17        59.02930369  85.00082653\n  46.88       143.12557289  80.8         86.66598109 207.64073281\n 397.23        80.98029528 117.77       117.33       101.63749557\n  62.65        86.11689047  76.59903555 124.49        60.08\n 103.41848446 230.6         43.4556468   82.96950594  85.57410146\n  52.69879882  53.42        36.09575685  53.10587067 208.77\n  45.31166756 248.18        52.15109978 179.39        40.06335019\n 106.52        40.52        36.53135678].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "# define the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C = 0.1, penalty=\"l2\", max_iter = 10000000)\n",
    "# fit the model on the whole dataset\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# predict a multinomial probability distribution\n",
    "#yhat = model.predict_proba()\n",
    "# summarize the predicted probabilities\n",
    "#print('Predicted Probabilities: %s' % yhat)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class:  ['NOS' 'NOS' 'HB' 'HCC' 'HB' 'NOS' 'HCC' 'NOS' 'HB' 'HB' 'NOS' 'HB' 'HCC'\n",
      " 'HB' 'HCC' 'NOS' 'HB' 'HCC' 'HB' 'HB' 'NOS' 'HCC' 'HB' 'HCC' 'HB' 'NOS'\n",
      " 'NOS' 'HB' 'HB' 'HCC' 'NOS' 'HB' 'HCC' 'HCC' 'HCC' 'NOS' 'HCC' 'HCC' 'HB'\n",
      " 'HCC' 'HB' 'HB' 'HCC' 'HB' 'HCC' 'HB' 'HCC' 'HCC' 'HB' 'HB' 'NOS' 'HB'\n",
      " 'HCC' 'HCC' 'HCC' 'HCC' 'HCC' 'HB' 'HB' 'HCC' 'HCC' 'HCC' 'HCC' 'HB' 'HB'\n",
      " 'HB' 'HCC' 'HCC' 'HCC' 'HCC' 'HB' 'HCC' 'HCC' 'HCC' 'HCC' 'HB' 'HCC']\n",
      "HCC    36\n",
      "HB     29\n",
      "NOS    12\n",
      "Name: 0, dtype: int64\n",
      "\n",
      "\n",
      "False    53\n",
      "True     24\n",
      "Name: Diagnosis_simplified, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3116883116883117"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the class label\n",
    "yhat = model.predict(test_X)\n",
    "# summarize the predicted class\n",
    "print(\"Predicted Class: \",  yhat)\n",
    "print(pd.DataFrame(yhat).iloc[:,0].value_counts())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(pd.DataFrame(yhat == test_y).iloc[:,0].value_counts())\n",
    "\n",
    "\n",
    "accuracy_score(test_y, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLR with nanostring_SR_prosp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (59, 229)\n",
      "HB     50\n",
      "HCC     7\n",
      "NOS     2\n",
      "Name: Diagnosis_simplified, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Subsets/Nanostring_SR_prosp.csv\", sep= \",\")\n",
    "subset_SR = df.loc[:, \"Diagnosis_simplified\": \"TBP_SR\"]\n",
    "subset_SR = subset_SR.dropna()\n",
    "print(\"shape:\",subset_SR.shape)\n",
    "print(subset_SR[\"Diagnosis_simplified\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sr = subset_SR.drop('Diagnosis_simplified', axis=1)\n",
    "y_sr = subset_SR['Diagnosis_simplified']\n",
    "bx_names = X.columns\n",
    "X_sr.columns = bx_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities: [[1.00000000e+000 2.09427050e-044 3.31906987e-035]\n",
      " [1.00000000e+000 1.59644594e-020 8.75676456e-087]\n",
      " [1.88141687e-048 7.52425917e-081 1.00000000e+000]\n",
      " [8.31816536e-016 1.00000000e+000 1.24921687e-018]\n",
      " [6.16573431e-007 9.99999383e-001 3.95911590e-123]\n",
      " [1.00000000e+000 6.16736722e-013 3.43632517e-160]\n",
      " [1.21548139e-020 8.77153070e-001 1.22846930e-001]\n",
      " [1.00000000e+000 2.39333098e-031 9.34310001e-148]\n",
      " [1.00000000e+000 8.79376012e-014 1.54937212e-031]\n",
      " [9.53376559e-006 9.99990466e-001 6.99907160e-197]\n",
      " [1.00000000e+000 1.37709051e-043 1.12654439e-078]\n",
      " [9.99999769e-001 2.31357951e-007 1.66743527e-035]\n",
      " [1.00000000e+000 2.37159758e-014 1.60188209e-094]\n",
      " [1.00000000e+000 8.84414152e-037 0.00000000e+000]\n",
      " [1.00000000e+000 3.51050566e-050 1.70224716e-273]\n",
      " [2.44616369e-006 9.99997554e-001 0.00000000e+000]\n",
      " [8.56090407e-003 9.91439096e-001 6.09732673e-015]\n",
      " [1.00000000e+000 8.03622770e-026 5.17820661e-063]\n",
      " [7.87012544e-012 1.00000000e+000 5.14998358e-013]\n",
      " [1.00000000e+000 5.69438231e-036 0.00000000e+000]\n",
      " [9.99815477e-001 1.84522707e-004 8.48044314e-011]\n",
      " [4.08757123e-004 9.99591243e-001 0.00000000e+000]\n",
      " [1.00000000e+000 1.83916310e-027 8.90693001e-092]\n",
      " [1.00000000e+000 4.09136058e-024 5.27855029e-105]\n",
      " [9.53360279e-001 4.66397208e-002 3.50508517e-135]\n",
      " [1.00000000e+000 2.11956249e-034 1.74405171e-070]\n",
      " [1.00000000e+000 2.91669742e-015 3.47477014e-041]\n",
      " [1.00000000e+000 1.10472717e-016 9.56699003e-068]\n",
      " [7.12799668e-058 1.00000000e+000 2.94125161e-051]\n",
      " [3.36950240e-001 6.63049760e-001 2.20778406e-106]\n",
      " [1.00000000e+000 1.96730964e-021 3.15842698e-067]\n",
      " [1.00000000e+000 8.02938724e-059 0.00000000e+000]\n",
      " [1.00000000e+000 1.24529555e-013 9.71265979e-174]\n",
      " [3.11072268e-049 1.00000000e+000 1.12081916e-154]\n",
      " [1.00000000e+000 3.48609644e-176 0.00000000e+000]\n",
      " [1.00000000e+000 7.67191572e-019 1.72854686e-107]\n",
      " [1.00000000e+000 1.70019638e-016 0.00000000e+000]\n",
      " [9.00033383e-001 9.99666169e-002 6.32477935e-019]\n",
      " [1.00000000e+000 3.04446228e-214 0.00000000e+000]\n",
      " [2.00435213e-029 1.00000000e+000 3.97337550e-272]\n",
      " [1.00000000e+000 1.98559147e-012 4.95418115e-229]\n",
      " [1.00000000e+000 2.11403381e-266 0.00000000e+000]\n",
      " [1.00000000e+000 3.28826098e-054 0.00000000e+000]\n",
      " [6.77727154e-094 2.51676394e-068 1.00000000e+000]\n",
      " [1.00000000e+000 6.15335938e-036 4.46231338e-077]\n",
      " [1.00000000e+000 9.49670860e-027 6.83116436e-033]\n",
      " [1.00000000e+000 5.78956622e-025 3.95416765e-122]\n",
      " [1.00000000e+000 4.06200569e-014 7.29159983e-103]\n",
      " [1.00000000e+000 1.86334374e-035 2.84110688e-252]\n",
      " [1.00000000e+000 4.59495251e-107 0.00000000e+000]\n",
      " [1.00000000e+000 1.12226573e-020 4.84589055e-087]\n",
      " [1.00000000e+000 7.82893905e-273 0.00000000e+000]\n",
      " [9.99994892e-001 5.10819427e-006 3.53107333e-065]\n",
      " [2.39351244e-128 1.00000000e+000 3.54574441e-107]\n",
      " [5.26091640e-010 9.99999999e-001 2.79545015e-034]\n",
      " [9.47017457e-001 5.76320302e-004 5.24062231e-002]\n",
      " [1.00000000e+000 2.07693850e-130 1.88146371e-286]\n",
      " [1.00000000e+000 1.52852553e-230 0.00000000e+000]\n",
      " [2.03745818e-001 7.96254169e-001 1.23161257e-008]]\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "# define the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C = 0.1, penalty=\"l2\", max_iter = 10000000)\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X_sm, y_sm)\n",
    "\n",
    "\n",
    "# predict a multinomial probability distribution\n",
    "yhat = model.predict_proba(X_sr)\n",
    "# summarize the predicted probabilities\n",
    "print('Predicted Probabilities: %s' % yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class:  ['HB' 'HB' 'NOS' 'HCC' 'HCC' 'HB' 'HCC' 'HB' 'HB' 'HCC' 'HB' 'HB' 'HB'\n",
      " 'HB' 'HB' 'HCC' 'HCC' 'HB' 'HCC' 'HB' 'HB' 'HCC' 'HB' 'HB' 'HB' 'HB' 'HB'\n",
      " 'HB' 'HCC' 'HCC' 'HB' 'HB' 'HB' 'HCC' 'HB' 'HB' 'HB' 'HB' 'HB' 'HCC' 'HB'\n",
      " 'HB' 'HB' 'NOS' 'HB' 'HB' 'HB' 'HB' 'HB' 'HB' 'HB' 'HB' 'HB' 'HCC' 'HCC'\n",
      " 'HB' 'HB' 'HB' 'HCC']\n",
      "HB     42\n",
      "HCC    15\n",
      "NOS     2\n",
      "Name: 0, dtype: int64\n",
      "\n",
      "\n",
      "True     43\n",
      "False    16\n",
      "Name: Diagnosis_simplified, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7288135593220338"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the class label\n",
    "yhat = model.predict(X_sr)\n",
    "# summarize the predicted class\n",
    "print(\"Predicted Class: \",  yhat)\n",
    "print(pd.DataFrame(yhat).iloc[:,0].value_counts())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(pd.DataFrame(yhat == y_sr).iloc[:,0].value_counts())\n",
    "\n",
    "accuracy_score(y_sr, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
